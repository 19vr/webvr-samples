<!doctype html>
<!--
Copyright 2016 The Chromium Authors. All rights reserved.
Use of this source code is governed by a BSD-style license that can be
found in the LICENSE file.
-->
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <title>04 - Simple Mirroring</title>

    <!--
      This sample demonstrates how to mirror content to an external display
      while presenting to a VRDisplay.
    -->

    <style>
      #webgl-canvas {
        box-sizing: border-box;
        height: 100%;
        left: 0;
        margin: 0;
        position: absolute;
        top: 0;
        width: 100%;
      }
    </style>

    <script src="js/third-party/gl-matrix-min.js"></script>

    <script src="js/third-party/wglu/wglu-program.js"></script>
    <script src="js/third-party/wglu/wglu-stats.js"></script>
    <script src="js/third-party/wglu/wglu-texture.js"></script>

    <script src="js/vr-cube-sea.js"></script>
    <script src="js/vr-samples-util.js"></script>
  </head>
  <body>
    <canvas id="webgl-canvas"></canvas>
    <script>
      /* global mat4, WGLUCubeSea, WGLUTextureLoader, VRSamplesUtil */
      (function () {
      "use strict";

      var vrDisplay = null;
      var projectionMat = mat4.create();
      var modelViewMat = mat4.create();

      // ================================
      // WebVR specific code begins here.
      // ================================

      // WebGL setup.
      var webglCanvas = document.getElementById("webgl-canvas");
      var gl = null;
      var cubeSea = null;
      var stats = null;

      function initWebGL(preserveDrawingBuffer) {
        // Setting preserveDrawingBuffer to true prevents the canvas from being
        // implicitly cleared when calling submitFrame or compositing the canvas
        // on the document. For the simplest form of mirroring we want to create
        // the canvas with that option enabled. Note that this may incur a
        // performance penalty, as it may imply that additional copies of the
        // canvas backbuffer need to be made. As a result, we ONLY want to set
        // that if we know the VRDisplay has an external display, which is why
        // we defer WebGL initialization until after we've gotten results back
        // from navigator.getVRDisplays and know which device we'll be
        // presenting with.
        gl = webglCanvas.getContext("webgl", { preserveDrawingBuffer: preserveDrawingBuffer });
        gl.clearColor(0.1, 0.2, 0.3, 1.0);
        gl.enable(gl.DEPTH_TEST);
        gl.enable(gl.CULL_FACE);

        var textureLoader = new WGLUTextureLoader(gl);
        var texture = textureLoader.loadTexture("media/textures/cube-sea.png");
        cubeSea = new VRCubeSea(gl, texture);
        stats = new WGLUStats(gl);

        // Wait until we have a WebGL context to resize and start rendering.
        window.addEventListener("resize", onResize, false);
        onResize();
        window.requestAnimationFrame(onAnimationFrame);
      }

      function onVRRequestPresent () {
        // This can only be called in response to a user gesture.
        vrDisplay.requestPresent({ source: webglCanvas }).then(function () {
          // Once we begin presenting, the canvas should be resized to the
          // recommended dimensions for the display.
          onResize();
        }, function () {
          VRSamplesUtil.addError("requestPresent failed.", 2000);
        });
      }

      if (navigator.getVRDisplays) {
        navigator.getVRDisplays().then(function (displays) {
          if (displays.length > 0) {
            vrDisplay = displays[0];

            VRSamplesUtil.addButton("Enter VR", "media/icons/cardboard64.png", onVRRequestPresent);

            // Only use preserveDrawingBuffer if we have an external display to
            // mirror to.
            initWebGL(vrDisplay.capabilities.hasExternalDisplay);
          } else {
            initWebGL(false);
          }
        });
      } else {
        // No VR means no mirroring, so create WebGL content without
        // preserveDrawingBuffer
        initWebGL(false);
        VRSamplesUtil.addError("Your browser does not support WebVR. See <a href='http://webvr.info'>webvr.info</a> for assistance.");
      }

      function onResize () {
        if (vrDisplay && vrDisplay.isPresenting) {
          // If we're presenting we want to use the drawing buffer size
          // recommended by the VRDevice, since that will ensure the best
          // results post-distortion.
          var leftEye = vrDisplay.getEyeParameters("left");
          var rightEye = vrDisplay.getEyeParameters("right");

          // For simplicity we're going to render both eyes at the same size,
          // even if one eye needs less resolution. You can render each eye at
          // the exact size it needs, but you'll need to adjust the viewports to
          // account for that.
          webglCanvas.width = Math.max(leftEye.renderWidth, rightEye.renderWidth) * 2;
          webglCanvas.height = Math.max(leftEye.renderHeight, rightEye.renderHeight);
        } else {
          // We only want to change the size of the canvas drawing buffer to
          // match the window dimensions when we're not presenting.
          webglCanvas.width = webglCanvas.offsetWidth * window.devicePixelRatio;
          webglCanvas.height = webglCanvas.offsetHeight * window.devicePixelRatio;
        }
      }

      function renderSceneView (pose, eye) {
        var orientation = pose.orientation;
        var position = pose.position;
        if (!orientation) { orientation = [0, 0, 0, 1]; }
        if (!position) { position = [0, 0, 0]; }

        if (eye)
          mat4.perspectiveFromFieldOfView(projectionMat, eye.fieldOfView, 0.1, 1024.0);
        else
          mat4.perspective(projectionMat, 45, webglCanvas.width / webglCanvas.height, 0.1, 1024.0);

        mat4.fromRotationTranslation(modelViewMat, orientation, position);
        if (eye)
          mat4.translate(modelViewMat, modelViewMat, eye.offset);
        mat4.invert(modelViewMat, modelViewMat);

        cubeSea.render(projectionMat, modelViewMat, stats);
      }

      function onAnimationFrame (t) {
        stats.begin();

        gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);

        if (vrDisplay) {
          vrDisplay.requestAnimationFrame(onAnimationFrame);

          var pose = vrDisplay.getPose();

          if(vrDisplay.isPresenting) {
            gl.viewport(0, 0, webglCanvas.width * 0.5, webglCanvas.height);
            renderSceneView(pose, vrDisplay.getEyeParameters("left"));

            gl.viewport(webglCanvas.width * 0.5, 0, webglCanvas.width * 0.5, webglCanvas.height);
            renderSceneView(pose, vrDisplay.getEyeParameters("right"));

            vrDisplay.submitFrame(pose);
          } else {
            gl.viewport(0, 0, webglCanvas.width, webglCanvas.height);
            renderSceneView(pose, null);
          }
        } else {
          window.requestAnimationFrame(onAnimationFrame);

          // No VRDisplay found.
          gl.viewport(0, 0, webglCanvas.width, webglCanvas.height);
          mat4.perspective(projectionMat, 45, webglCanvas.width / webglCanvas.height, 0.1, 1024.0);
          mat4.identity(modelViewMat);
          cubeSea.render(projectionMat, modelViewMat, stats);

          stats.renderOrtho();
        }

        stats.end();
      }
      })();
    </script>
  </body>
</html>
